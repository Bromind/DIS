\section{Experiments}
To assess the efficiency of our algorithms, we consider the task handling problem where 3 types of tasks appear in a closed arena. A task, represented as a colored cylinder (with colors being red, green or blue), is processed by a robot being in its vicinity for a given amount of time, but is not processed faster as the number of robots close to its position increases. Once a task is processed, it disappears and a new one appears at a random location. The goal of our algorithms is to optimize the task allocation so as to prevent multiple robots from picking the same task.

Robots identify the tasks with the onboard camera. Each robot evaluates stimuli for red, green and blue tasks based on its local camera observations (perceived colors and pixel count, relative sizes in number of pixels). The stimuli  for  red,  green  and blue  are  multiplied  by  weights $w_r$,  $w_g$,  $w_b$. The  robot  will  then select the  task according to our thresholds-based algorithms. Once a  robot reaches  a  task it stops until the task is processed and disappears.

In order to emit the virtual potential field, the robots are endowed with radio emitter (and receiver) that broadcasts (and gathers) the relevant information perceived by the robots, within a short range. These information are then fed into our public and individual threshold-based algorithm to dynamically adjust the weights of the stimuli.

\textbf{Implementation:}
The framework used during the course of the project allowed us to test 3 main approaches. They could be divided in two categories, one being private (static case) and the other being public (dynamic case). On the one hand, we considered both homogenous (fixed and identical thresholds among robots) and heterogeneous (adative thresholds based on each robot's vision or time spent in search mode) cases. On the other hand, we took advantage of the local potential field emissions perceived by the robots to adjust the weights of each color stimulus to improve task allocation and tried both fixed and variable thresholds.

\textbf{Task Allocation Mechanism:}
For the simulations to be comparable, we used the same Finite State Machine (FSM) for all simulations and all types of algorithms. The behavior of the robots could be described as follows. All the robots are initialized in the first state that consists in spinning around and processing the image from the camera to determine if task needs to be handled. Once a task is picked, the robot changes state and goes directly towards the task. If the robot happens to lose track of the target (when the stimulus gets lower than a given threshold), it switches back to the search state. When the robot is close enough to the task, it slows down for a given amount of time to process the task.

\subsection{Private, Fixed-Threshold Algorithm}
In this first experiment, the same response threshold is assigned to each robot and identical for each color (no specialization). Various agent behaviors are obtained thanks to the local perception of the environment and the private assessment of the demand. We considered both the deterministic and probabilistic task allocation. In the latter, the robots where using a sigmoid function to determine if a given task should be processed or not.

\subsection{Private, Adaptive-Threshold Algorithm}
The next case considers adaptive thresholds to improve the robustness of the task allocation strategy. As the number of robots is not equal nor a multiple of the number of task categories, we chose to consider two adaptation methods. The first one considers the robots to be "colorblind", the threshold is the same for each color and the adaptation consists in lowering the value of all the thresholds as the robots remain in search mode (and conversely, increasing the value if a task is performed). The second one proceeds to a specialization of the robots as they perform a given type of task repeatedly. This approach can be combined with the previous one by lowering the thresholds of the robots if too much time is spent in search mode.

\subsection{Public, Fixed \& Variable Threshold Algorithm}
The final approach adds communication between the robots to share information about the tasks identified through vision. The method consists in using the information received through radio communication to change the weights of the color stimuli so as to avoid selecting the same task as another robot.

In order to neglect the influence of neighbors being too far from a robot position, we chose to limit the maximum range at which the emissions were perceived. The value was set so that the corresponding area of reception of a given robot was one fifth of the total area of the arena. Thus, each robot was more inclined to work in its own area and could deal with neighbors entering the area and likely to choose the same task. Furthermore, we chose to modify the saturate the signal strength of the received emissions. Indeed, for the robot to react sufficiently fast if it picked the same task as its neighbor, the signal strength had to be limited to a maximum value before normalization of the received perceptions. This assumption somehow goes beyond the initial idea of potential field but is perfectly workable using radio communications. It should also be noted that the model adopted here could be applicable to really noisy and imprecise receiver.

The sent information had to be meaningful for the robots to assess the situation correctly. Although we tried to send the proximity of the closest task for each color, we eventually restrained the information sent to the closeness of the chosen task. This information was then updated and sent as long as the robot was travelling to the task. Based on this information, the robots could decide to search for a new task if the same color was chosen by a neighbor that was closer to the task.

In this project, we propose a function to update each color stimulus based on the information perceived by the neighbors. This function is comprised of two factors: the weight of local stimuli, $\alpha$, and the weight of neighbors stimuli, $\beta$. The first factor reduces the power of the stimuli as the received values increases. The second one allows the robot to still have a chance to perform the task it had chosen if it is closer to than its neighbors. If the decrease in the stimulus is high enough (i.e. the stimulus is less than a given threshold, called lost threshold), the robot will abandon the task it had chosen and resume search.

$$
S_{new}(i) = S_{prev}(i)*(1 + \beta) - \alpha*R(i) \eqno{(1)}
$$
